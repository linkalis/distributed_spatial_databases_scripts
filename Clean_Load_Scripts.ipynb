{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Clean, and Load Data Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, fnmatch\n",
    "import json\n",
    "from bson import ObjectId\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import pymongo\n",
    "\n",
    "from neo4j.v1 import GraphDatabase\n",
    "from neo4j.v1 import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/'\n",
    "logs_folder = '/Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    ''' Takes a folder name and a logs directory path and initializes a log file containing the name of \n",
    "    every file in the target folder.  Contains methods for checking which files in the log have not yet \n",
    "    been loaded and getting and reading in the next available file. '''\n",
    "    \n",
    "    def __init__(self, data_path, logs_path):\n",
    "        self.data_path = data_path\n",
    "        self.logs_path = logs_path\n",
    "        \n",
    "        # Create a directory to store the log files, if necessary\n",
    "        logs_dir = os.path.dirname(self.logs_path)\n",
    "        if not os.path.exists(logs_dir):\n",
    "            os.makedirs(logs_dir)\n",
    "        \n",
    "        # Create a 'files_to_load.txt' file, then write the name of every file in the directory to this file\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"w\")\n",
    "        data_files_list = os.listdir(self.data_path) \n",
    "        file_type = \"*.txt\"  \n",
    "        for file in data_files_list:  \n",
    "            if fnmatch.fnmatch(file, file_type):\n",
    "                files_to_load_log.write(file)\n",
    "                files_to_load_log.write(\"\\n\")\n",
    "        files_to_load_log.close()     \n",
    "\n",
    "    def next_file_available(self):\n",
    "        ''' Checks if there's another file available in the files_to_load.txt log. If there's a \n",
    "        file available, returns True. If no files are remaining, returns False so we can stop \n",
    "        reading in data. '''\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r\")\n",
    "        next_file_name = files_to_load_log.readline().rstrip(\"\\n\")\n",
    "        if next_file_name == '':\n",
    "            return(False)\n",
    "        else:\n",
    "            return(True)\n",
    "        \n",
    "    def get_next_file(self):\n",
    "        ''' Reads from the files_to_load.txt file and gets the name of the next file in the list.\n",
    "        Calls read_file() to read in the target file in as list of dictionaries. Returns a tuple \n",
    "        that includes the list of dictionaries representing the JSON data, along with the filename\n",
    "        so we can keep track of this file in subsequent tasks. '''\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r\")\n",
    "        next_file_name = files_to_load_log.readline().rstrip(\"\\n\") # strip the newline character from the end of filename\n",
    "        print(\"Extractor: Next file is: \" + next_file_name)\n",
    "        next_file_path = self.data_path + next_file_name \n",
    "        return(self.read_data_file(next_file_path), next_file_name)\n",
    "        \n",
    "    def read_data_file(self, file_to_read):\n",
    "        ''' Reads the JSON-formatted file line by line and returns each line as a dictionary. '''\n",
    "        print(\"Extractor: Reading file: \" + file_to_read)\n",
    "        reading_file = open(file_to_read, \"r\") # open as read-only\n",
    "        list_of_jsondicts = []\n",
    "        for line in reading_file.readlines():\n",
    "            list_of_jsondicts.append(json.loads(line))\n",
    "        print(\"Extractor: Read \" + str(len(list_of_jsondicts)) + \" data rows.\")\n",
    "        return(list_of_jsondicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Cleaner: \n",
    "    ''' Takes a batch of data that's been extracted as a list of dictionaries.  Contains methods to \n",
    "    iterate over each record in the list, running it through a series of cleaning steps. Logs the\n",
    "    ids of the records that are affected by the various cleaning steps. Returns the cleaned data back \n",
    "    as a list. '''\n",
    "    \n",
    "    def __init__(self, data_list, file_name, logs_path):\n",
    "        self.data_list = data_list\n",
    "        self.logs_path = logs_path\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    def clean_data(self):\n",
    "        ''' Iterates over each data element, progressing through each cleaning step on each element. \n",
    "        Logs the ids of data elements that contain nulls and/or errors to arrays as we go. At the end\n",
    "        of cleaning, invokes the log_cleaning() method to '''\n",
    "        \n",
    "        step1_log = []\n",
    "        step2_log = []\n",
    "        \n",
    "        #i = 0\n",
    "        for record in self.data_list:\n",
    "            #print(i)\n",
    "            self.fix_null_places(record, step1_log)\n",
    "            self.fix_bounding_box(record, step2_log)\n",
    "            self.get_centroid(record)\n",
    "            #i += 1\n",
    "        \n",
    "        print(\"Cleaner: Finished cleaning records.\")\n",
    "        self.log_cleaning(step1_log, step2_log)\n",
    "        return(self.data_list)\n",
    "        \n",
    "    def fix_null_places(self, record, log_array):\n",
    "        ''' Since place values are critical to our data model, substitute dummy \n",
    "        values if we have a place value that equals 'None'. This will keep it from \n",
    "        blowing up the database when we try to insert. '''\n",
    "        \n",
    "        if record['place'] is None:\n",
    "            record['place'] = dict()\n",
    "            record['place']['id'] = \"9999999\"\n",
    "            record['place']['name'] = \"No Place\"\n",
    "            record['place']['full_name'] = \"No Place Available\"\n",
    "            record['place']['country'] = \"No Country Available\"\n",
    "            record['place']['country_code'] = \"ZZ\"\n",
    "            record['place']['place_type'] = \"NA\"\n",
    "            record['place']['url'] = \"NA\"\n",
    "            record['place']['bounding_box'] = dict() # initialize dictionary to hold bounding box\n",
    "            record['place']['bounding_box']['type'] = \"Polygon\"\n",
    "            record['place']['bounding_box']['coordinates'] = list() # initialize coordinate list w/in bounding box\n",
    "            record['place']['bounding_box']['coordinates'].append([]) # append the [0] element to hold four pairs of coordinates\n",
    "            record['place']['bounding_box']['coordinates'][0].append([0.0, 0.0]) # append 'dummy' coordinates\n",
    "            record['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            record['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            record['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            log_array.append(record[\"id_str\"])\n",
    "       \n",
    "    def fix_bounding_box(self, record, log_array):\n",
    "        ''' Fix a few issues that are going on with bounding boxes:\n",
    "        1. Twitter Place bounding boxes only have four points. Need to close them off so they're a \n",
    "        complete polygon. Take the first coordinate of the bounding box array and repeat it at the \n",
    "        end of the bounding box array.\n",
    "        2. If the bounding box is actually a point (i.e. all of the four points are the same), then \n",
    "        \"fake out\" a bounding box by transforming into a small rectangle with a small buffer around \n",
    "        the point.  We can recognize these by looking for place.place_type == 'poi'. '''\n",
    "        \n",
    "        #print(record['id_str'])\n",
    "        original_bounding_box = record['place']['bounding_box']['coordinates'][0].copy()\n",
    "        #print(original_bounding_box)\n",
    "        #print(record['place']['place_type'])\n",
    "        \n",
    "        if (record['place']['place_type'] == 'poi' or record['place']['place_type'] == 'NA'):\n",
    "            point_bounding_box = [[None for x in range(2)] for y in range(5)]\n",
    "            point_bounding_box[0][0] = original_bounding_box[0][0] - 0.0001\n",
    "            point_bounding_box[0][1] = original_bounding_box[0][1] - 0.0001\n",
    "            point_bounding_box[1][0] = original_bounding_box[1][0] - 0.0001\n",
    "            point_bounding_box[1][1] = original_bounding_box[1][1] + 0.0001\n",
    "            point_bounding_box[2][0] = original_bounding_box[2][0] + 0.0001\n",
    "            point_bounding_box[2][1] = original_bounding_box[2][1] + 0.0001\n",
    "            point_bounding_box[3][0] = original_bounding_box[3][0] + 0.0001\n",
    "            point_bounding_box[3][1] = original_bounding_box[3][1] - 0.0001\n",
    "            point_bounding_box[4][0] = original_bounding_box[0][0] - 0.0001\n",
    "            point_bounding_box[4][1] = original_bounding_box[0][1] - 0.0001\n",
    "            record['place']['better_bounding_box'] = dict()\n",
    "            record['place']['better_bounding_box']['type'] = \"Polygon\"\n",
    "            record['place']['better_bounding_box']['coordinates'] = list()\n",
    "            record['place']['better_bounding_box']['coordinates'].append([])\n",
    "            record['place']['better_bounding_box']['coordinates'][0] = point_bounding_box\n",
    "            #print(record['place']['better_bounding_box']['coordinates'])\n",
    "            log_array.append(record[\"id_str\"])\n",
    "        else:\n",
    "            first_coords = original_bounding_box[0]\n",
    "            #print(first_coords)\n",
    "            original_bounding_box.append(first_coords)\n",
    "            #print(original_bounding_box)\n",
    "            record['place']['better_bounding_box'] = dict()\n",
    "            record['place']['better_bounding_box']['type'] = \"Polygon\"\n",
    "            record['place']['better_bounding_box']['coordinates'] = list()\n",
    "            record['place']['better_bounding_box']['coordinates'].append([])\n",
    "            record['place']['better_bounding_box']['coordinates'][0] = original_bounding_box\n",
    "            #print(record['place']['better_bounding_box']['coordinates'])\n",
    "                  \n",
    "    def get_centroid(self, record):\n",
    "        bounding_box = record['place']['better_bounding_box']['coordinates'][0];\n",
    "        lower_left = bounding_box[0];\n",
    "        upper_right = bounding_box[2];\n",
    "        centroid_long = lower_left[0] + ((upper_right[0] - lower_left[0]) / 2);\n",
    "        centroid_lat = lower_left[1] + ((upper_right[1] - lower_left[1]) / 2);\n",
    "        record['place']['centroid'] = dict()\n",
    "        record['place']['centroid']['type'] = \"Point\"\n",
    "        record['place']['centroid']['coordinates'] = [centroid_long, centroid_lat]\n",
    "        \n",
    "    def log_cleaning(self, step1_log, step2_log):\n",
    "        ''' When cleaning is done, put the cleaning log arrays into a dictionary and write the result \n",
    "        to the cleaning log file. '''\n",
    "        \n",
    "        log_dict = dict()\n",
    "        log_dict['file_name'] = self.file_name\n",
    "        log_dict['null_places_fixed'] = step1_log\n",
    "        log_dict['bounding_boxes_fixed'] = step2_log\n",
    "        \n",
    "        cleaning_log  = open(self.logs_path + \"/cleaning_log.txt\", \"a+\") # open file in append mode\n",
    "        cleaning_log.write(json.dumps(log_dict))\n",
    "        cleaning_log.write(\"\\n\")\n",
    "        cleaning_log.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader\n",
    "\n",
    "https://neo4j.com/developer/python/\n",
    "\n",
    "https://www.lynda.com/Neo4j-tutorials/Use-Neo4j-driver-Python/601789/659331-4.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    ''' \n",
    "    Contains general methods for initializing a database connection, loading data by interating over\n",
    "    records and writing them one by one to the database, and logging data about the number of successful\n",
    "    and failed loads to a log file. When setting up the database connection, this class invokes other\n",
    "    database-specific loader classes that contain all required methods to \"plug and play\" with this \n",
    "    generic loader class (ex: initialize_connection(), load_record(), etc.). '''\n",
    "    \n",
    "    def __init__(self, data_list, file_name, logs_path):\n",
    "        self.data_list = data_list\n",
    "        self.logs_path = logs_path\n",
    "        self.file_name = file_name\n",
    "        self.db_connection = None\n",
    "    \n",
    "    def get_connection(self, db_type):\n",
    "        if db_type == \"neo4j\":\n",
    "            self.db_connection = Neo4jLoader()\n",
    "            self.db_connection.initialize_connection()\n",
    "        if db_type == \"mongodb\":\n",
    "            self.db_connection = MongoDBLoader()\n",
    "            self.db_connection.initialize_connection()\n",
    "    \n",
    "    def load_data(self):\n",
    "        # Initialize variables we want to count so we can output them to the log file at the end of load\n",
    "        begin = time.time()\n",
    "        i = 0\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        fail_log = []\n",
    "        \n",
    "        print(\"Loader: Loading records...\")\n",
    "        for record in self.data_list:\n",
    "            #print(i)\n",
    "            #print(\"Loading record with id: \" + record['id_str'] + \"; User id: \" + str(record['user']['id']) + \"; Place id: \" + str(record['place']['id']))\n",
    "            try:\n",
    "                self.db_connection.load_record(record)\n",
    "                success_count += 1\n",
    "                #print(\"Loaded!\")\n",
    "            except Exception as e:\n",
    "                print(\"Couldn't load record with id: \" + record['id_str'])\n",
    "                #print(e)\n",
    "                fail_count += 1\n",
    "                fail_dict = dict()\n",
    "                fail_dict['id'] = record['id_str']\n",
    "                fail_dict['error'] = str(e)\n",
    "                fail_log.append(fail_dict)      \n",
    "            #i += 1\n",
    "        \n",
    "        print(\"Loader: Finished loading records.\")\n",
    "        end = time.time()\n",
    "        load_time = end - begin # compute time elapsed for load\n",
    "        self.db_connection.close_connection() # close database connection\n",
    "        self.log_load(load_time, success_count, fail_count, fail_log) # write load results to log\n",
    "    \n",
    "    def log_load(self, load_time, success_count, fail_count, fail_log):\n",
    "        ''' When load is done, record the time it took to run, number of successes, and number of failures.\n",
    "        Write this info, along with file_name, as a JSON string to a log file. Then remove the name of the \n",
    "        successfully loaded file from files_to_load.txt so we don't try to re-load it on the next iteration. ''' \n",
    "        \n",
    "        log_dict = dict()\n",
    "        log_dict['file_name'] = self.file_name\n",
    "        log_dict['load_time'] = load_time\n",
    "        log_dict['success_count'] = success_count\n",
    "        log_dict['fail_count'] = fail_count\n",
    "        log_dict['fail_log'] = fail_log\n",
    "        loaded_files_log  = open(self.logs_path + \"/loaded_files.txt\", \"a+\") # open file in append mode\n",
    "        loaded_files_log.write(json.dumps(log_dict))\n",
    "        loaded_files_log.write(\"\\n\")\n",
    "        loaded_files_log.close()\n",
    "        \n",
    "        # https://www.reddit.com/r/learnpython/comments/3xuych/least_resource_intensive_way_to_delete_first_line/\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r+\") # open in read/write mode\n",
    "        files_to_load_log.readline() # read the first line and throw it out\n",
    "        remaining_files = files_to_load_log.read() # read the rest\n",
    "        files_to_load_log.seek(0) # set the cursor to the top of the file\n",
    "        files_to_load_log.write(remaining_files) # write the data back\n",
    "        files_to_load_log.truncate() # set the file size to the current size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Neo4jLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connection = None\n",
    "        \n",
    "        self.neo4j_query_string = \"\"\"\n",
    "                MERGE (t:Tweet {tweet_id: toInteger($tweet_id)})\n",
    "                ON CREATE SET t.text = $text,\n",
    "                    t.lang = $lang,\n",
    "                    t.timestamp_ms = toInteger($timestamp_ms),\n",
    "                    t.favorited = $favorited,\n",
    "                    t.retweeted = $retweeted,\n",
    "                    t.retweet_count = toInteger($retweet_count),\n",
    "                    t.favorite_count = toInteger($favorite_count),\n",
    "                    t.quote_count = toInteger($quote_count),\n",
    "                    t.reply_count = toInteger($reply_count),\n",
    "                    t.coordinates = point({ \n",
    "                        longitude: toFloat($tweet_coordinates_long), \n",
    "                        latitude: toFloat($tweet_coordinates_lat) \n",
    "                    })\n",
    "\n",
    "                MERGE (u:User {user_id: toInteger($user_id)})\n",
    "                SET\tu.name = $user_name,\n",
    "                    u.screen_name = $user_screen_name,\n",
    "                    u.description = $user_description,\n",
    "                    u.location = $user_location,\n",
    "                    u.lang = $user_lang,\n",
    "                    u.time_zone = $user_time_zone,\n",
    "                    u.verified = $user_verified,\n",
    "                    u.utc_offset = $user_utc_offset,\n",
    "                    u.created_at = $user_created_at,\n",
    "                    u.listed_count = $user_listed_count,\n",
    "                    u.friends_count = $user_friends_count,\n",
    "                    u.followers_count = $user_followers_count,\n",
    "                    u.favourites_count = $user_favourites_count,\n",
    "                    u.is_translator = $user_is_translator,\n",
    "                    u.statuses_count = $user_statuses_count\n",
    "                    \n",
    "\n",
    "                MERGE (t)-[:TWEETED_BY]->(u)\n",
    "                MERGE (u)-[:TWEETED]->(t)\n",
    "                \n",
    "                MERGE (p:Place {place_id: toString($place_id)})\n",
    "                SET\tp.name = $place_name,\n",
    "                    p.full_name = $place_full_name,\n",
    "                    p.country = $place_country,\n",
    "                    p.country_code = $place_country_code,\n",
    "                    p.place_type = $place_type,\n",
    "                    p.bounding_box_LL = point({ \n",
    "                        longitude: toFloat($place_bounding_box_LL_long), \n",
    "                        latitude: toFloat($place_bounding_box_LL_lat) \n",
    "                    }),\n",
    "                    p.bounding_box_UR = point({ \n",
    "                        longitude: toFloat($place_bounding_box_UR_long), \n",
    "                        latitude: toFloat($place_bounding_box_UR_lat) \n",
    "                    }),\n",
    "                    p.centroid = point({ \n",
    "                        longitude: toFloat($place_centroid_long), \n",
    "                        latitude: toFloat($place_centroid_lat) \n",
    "                    })\n",
    "                \n",
    "                MERGE (t)-[:LOCATED_AT]->(p)\n",
    "                \n",
    "                WITH t, $entities_user_mentions AS mentions\n",
    "                UNWIND mentions AS mention\n",
    "                    MERGE (mentioned_user:User {user_id: toInteger(mention.id), name: mention.name, screen_name: mention.screen_name})\n",
    "                    MERGE (t)-[:MENTIONS]->(mentioned_user)\n",
    "\n",
    "                WITH t, $entities_hashtags AS hashtags\n",
    "                UNWIND hashtags AS hashtag\n",
    "                    MERGE (h:Hashtag {hashtag_id: hashtag.text})\n",
    "                    MERGE (t)-[:HASHTAGS]->(h)\n",
    "                \"\"\"\n",
    "    \n",
    "    def initialize_connection(self):\n",
    "        # Initialize Neo4j driver\n",
    "        uri = \"bolt://localhost:7687\"\n",
    "        #user = input(\"Username: \")\n",
    "        #pwd = input(\"Password: \")\n",
    "        driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"n0sql4m3\"))\n",
    "        self.connection = driver\n",
    "        #return(driver)\n",
    "    \n",
    "    def load_record(self, record):\n",
    "        with self.connection.session() as session:\n",
    "            tx = session.begin_transaction()\n",
    "            #print(type(tx))\n",
    "            db_result = self.structure_data_for_load(tx, record)\n",
    "            tx.commit()\n",
    "    \n",
    "    #@staticmethod\n",
    "    def structure_data_for_load(self, tx, data_element):\n",
    "        tx.run(self.neo4j_query_string, parameters={\n",
    "            'tweet_id': data_element['id_str'],\n",
    "            'text': data_element['text'],\n",
    "            'lang': data_element['lang'],\n",
    "            'timestamp_ms': data_element['timestamp_ms'],\n",
    "            'favorited': data_element['favorited'],\n",
    "            'retweeted': data_element['retweeted'],\n",
    "            'retweet_count': data_element['retweet_count'],\n",
    "            'favorite_count': data_element['favorite_count'],\n",
    "            'quote_count': data_element['quote_count'],\n",
    "            'reply_count': data_element['reply_count'],\n",
    "            'tweet_coordinates_long': data_element['coordinates']['coordinates'][0] if data_element['coordinates'] != None else None,\n",
    "            'tweet_coordinates_lat': data_element['coordinates']['coordinates'][1] if data_element['coordinates'] != None else None,\n",
    "            'user_id': data_element['user']['id'],\n",
    "            'user_name': data_element['user']['name'],\n",
    "            'user_screen_name': data_element['user']['screen_name'],\n",
    "            'user_description': data_element['user']['description'],\n",
    "            'user_location': data_element['user']['location'],\n",
    "            'user_lang': data_element['user']['lang'],\n",
    "            'user_time_zone': data_element['user']['time_zone'],\n",
    "            'user_verified': data_element['user']['verified'],\n",
    "            'user_utc_offset': data_element['user']['utc_offset'],\n",
    "            'user_created_at': data_element['user']['created_at'],\n",
    "            'user_listed_count': data_element['user']['listed_count'],\n",
    "            'user_friends_count': data_element['user']['friends_count'],\n",
    "            'user_followers_count': data_element['user']['followers_count'],\n",
    "            'user_favourites_count': data_element['user']['favourites_count'],\n",
    "            'user_is_translator': data_element['user']['is_translator'],\n",
    "            'user_statuses_count': data_element['user']['statuses_count'],\n",
    "            'place_id': data_element['place']['id'],\n",
    "            'place_name': data_element['place']['name'],\n",
    "            'place_full_name': data_element['place']['full_name'],\n",
    "            'place_country': data_element['place']['country'],\n",
    "            'place_country_code': data_element['place']['country_code'],\n",
    "            'place_type': data_element['place']['place_type'],\n",
    "            'place_bounding_box_LL_long': data_element['place']['better_bounding_box']['coordinates'][0][0][0],\n",
    "            'place_bounding_box_LL_lat': data_element['place']['better_bounding_box']['coordinates'][0][0][1],\n",
    "            'place_bounding_box_UR_long': data_element['place']['better_bounding_box']['coordinates'][0][2][0],\n",
    "            'place_bounding_box_UR_lat': data_element['place']['better_bounding_box']['coordinates'][0][2][1],\n",
    "            'place_centroid_long': data_element['place']['centroid']['coordinates'][0],\n",
    "            'place_centroid_lat': data_element['place']['centroid']['coordinates'][1],\n",
    "            'entities_user_mentions': data_element['entities']['user_mentions'],\n",
    "            'entities_hashtags': data_element['entities']['hashtags']\n",
    "        })\n",
    "        \n",
    "    def close_connection(self):\n",
    "        self.connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MongoDBLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connection = None\n",
    "        self.client = None\n",
    "    \n",
    "    def initialize_connection(self):\n",
    "        self.client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "        target_db = self.client.twitter_test\n",
    "        target_collection = target_db.tweets\n",
    "        # Initialize index on tweet 'id' field so we throw an error when trying to load duplicates of the same tweet\n",
    "        target_collection.create_index([(\"id\", pymongo.ASCENDING)], name='id_index', unique=True) \n",
    "        self.connection = target_collection\n",
    "    \n",
    "    def load_record(self, record):\n",
    "        record_id = str(record['id_str'])\n",
    "        #self.connection.update_one({id: record_id}, {\"$set\": {\"id\": record_id}}, upsert=True)\n",
    "        self.connection.insert_one(record)\n",
    "    \n",
    "    def close_connection(self):\n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = Extractor(data_folder, logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while extractor.next_file_available():\n",
    "    next_file_data, next_file_name = extractor.get_next_file() # read in the next file\n",
    "    cleaner = Cleaner(next_file_data, next_file_name, logs_folder)\n",
    "    cleaned_data = cleaner.clean_data()\n",
    "    loader = Loader(cleaned_data, next_file_name, logs_folder)\n",
    "    loader.get_connection(\"neo4j\")\n",
    "    loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor = Extractor(data_folder, logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "next_file_data, next_file_name = extractor.get_next_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaner = Cleaner(next_file_data, next_file_name, logs_folder)\n",
    "cleaned_data = cleaner.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#next_file_data[122]['id_str'] # has coordinates and a place\n",
    "pprint.pprint(cleaned_data[576]['place']['centroid']['coordinates'])\n",
    "pprint.pprint(cleaned_data[4234]['place']['centroid']['coordinates'])\n",
    "pprint.pprint(cleaned_data[575]['place']['centroid']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(cleaned_data[1243])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = Loader(cleaned_data, next_file_name, logs_folder)\n",
    "loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing Neo4j import\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = input(\"Username: \")\n",
    "pwd = input(\"Password: \")\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"MATCH (n:Tweet) RETURN n LIMIT 25\")\n",
    "    \n",
    "    for record in result:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/geo/places-near-location/api-reference/get-geo-reverse_geocode.html\n",
    "\n",
    "twurl '/1.1/geo/reverse_geocode.json?lat=2.204446&long=102.189931&granularity=country'\n",
    "twurl '/1.1/geo/reverse_geocode.json?lat=2.255562&long=102.250785&granularity=country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
