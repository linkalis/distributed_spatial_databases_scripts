{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON manipulation: https://realpython.com/python-json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, fnmatch\n",
    "import json\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "from neo4j.v1 import GraphDatabase\n",
    "from neo4j.v1 import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/'\n",
    "logs_folder = '/Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    ''' Takes a folder name and a logs directory path and initializes a log file containing the name of \n",
    "    every file in the target folder. The get_next_file() method gets the next file in the folder that \n",
    "    hasn't yet been loaded into the database. It then reads in the next file and returns it as \n",
    "    list of dictionary objects for further manipulation. '''\n",
    "    \n",
    "    def __init__(self, data_path, logs_path):\n",
    "        self.data_path = data_path\n",
    "        self.logs_path = logs_path\n",
    "        \n",
    "        # Create a directory to store the log files, if necessary\n",
    "        logs_dir = os.path.dirname(self.logs_path)\n",
    "        if not os.path.exists(logs_dir):\n",
    "            os.makedirs(logs_dir)\n",
    "        \n",
    "        # Create a 'files_to_load.txt' file, then write the name of every file in the directory to this file\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"w\")\n",
    "        data_files_list = os.listdir(self.data_path) \n",
    "        file_type = \"*.txt\"  \n",
    "        for file in data_files_list:  \n",
    "            if fnmatch.fnmatch(file, file_type):\n",
    "                files_to_load_log.write(file)\n",
    "                files_to_load_log.write(\"\\n\")\n",
    "        files_to_load_log.close()     \n",
    "\n",
    "    def next_file_available(self):\n",
    "        ''' Check if there's another file available in the files_to_load.txt log. If no files are\n",
    "        remaining, return False so we can stop reading in data. '''\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r\")\n",
    "        next_file_name = files_to_load_log.readline().rstrip(\"\\n\")\n",
    "        if next_file_name == '':\n",
    "            return(False)\n",
    "        else:\n",
    "            return(True)\n",
    "        \n",
    "    def get_next_file(self):\n",
    "        ''' Reads from the files_to_load.txt file and gets the name of the next file in the list.\n",
    "        Calls read_file() to read in the target file in as list of dictionaries. Returns a tuple \n",
    "        that includes the list of dictionaries representing the JSON data, along with the filename\n",
    "        so we can keep track of this file in subsequent tasks. '''\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r\")\n",
    "        next_file_name = files_to_load_log.readline().rstrip(\"\\n\") # strip the newline character from the end of filename\n",
    "        print(\"Next file is: \" + next_file_name)\n",
    "        next_file_path = self.data_path + next_file_name \n",
    "        return(self.read_data_file(next_file_path), next_file_name)\n",
    "        \n",
    "    def read_data_file(self, file_to_read):\n",
    "        ''' Reads the JSON-formatted file line by line and returns each line as a dictionary. '''\n",
    "        print(\"Reading file: \" + file_to_read)\n",
    "        reading_file = open(file_to_read, \"r\") # open as read-only\n",
    "        list_of_jsondicts = []\n",
    "        for line in reading_file.readlines():\n",
    "            list_of_jsondicts.append(json.loads(line))\n",
    "        print(\"Read \" + str(len(list_of_jsondicts)) + \" data rows.\")\n",
    "        return(list_of_jsondicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor = Extractor(data_folder, logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next file is: 500M_unicode_splitbb.txt\n",
      "Reading file: /Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/500M_unicode_splitbb.txt\n",
      "Read 5000 data rows.\n"
     ]
    }
   ],
   "source": [
    "next_file_data, next_file_name = extractor.get_next_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Cleaner: \n",
    "    ''' Takes a batch of data that's been extracted as a list of dictionaries. \n",
    "    The clean_data() method iterates over each data element in the list, running it \n",
    "    through a series of cleaning steps. Returns the cleaned data back as a list. '''\n",
    "    \n",
    "    def __init__(self, data_list, file_name, logs_path):\n",
    "        self.data_list = data_list\n",
    "        self.logs_path = logs_path\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    def clean_data(self):\n",
    "        ''' Iterate over each data element, progressing through each cleaning step on each element. \n",
    "        Log the ids of data elements that contain nulls and/or errors to the cleaning log as we go. '''\n",
    "        step1_log = []\n",
    "        step2_log = []\n",
    "        \n",
    "        #i = 0\n",
    "        for data_element in self.data_list:\n",
    "            #print(i)\n",
    "            self.fix_null_places(data_element, step1_log)\n",
    "            self.fix_bounding_box(data_element, step2_log)\n",
    "            self.get_centroid(data_element)\n",
    "            #i += 1\n",
    "        \n",
    "        # Put the cleaning log arrays into a dictionary and write the result to the cleaning log file.\n",
    "        log_dict = dict()\n",
    "        log_dict['file_name'] = self.file_name\n",
    "        log_dict['null_places_fixed'] = step1_log\n",
    "        log_dict['bounding_boxes_fixed'] = step2_log\n",
    "        cleaning_log  = open(self.logs_path + \"/cleaning_log.txt\", \"a+\") # open file in append mode\n",
    "        cleaning_log.write(json.dumps(log_dict))\n",
    "        cleaning_log.write(\"\\n\")\n",
    "        cleaning_log.close()\n",
    "        \n",
    "        return(self.data_list)\n",
    "        \n",
    "    def fix_null_places(self, data_element, log_array):\n",
    "        ''' Since place values are critical to our data model, substitute dummy \n",
    "        values if we have a place value that equals 'None'. This will keep it from \n",
    "        blowing up the database when we try to insert.\n",
    "        '''\n",
    "        if data_element['place'] is None:\n",
    "            data_element['place'] = dict()\n",
    "            data_element['place']['id'] = \"9999999\"\n",
    "            data_element['place']['name'] = \"No Place\"\n",
    "            data_element['place']['full_name'] = \"No Place Available\"\n",
    "            data_element['place']['country'] = \"No Country Available\"\n",
    "            data_element['place']['country_code'] = \"ZZ\"\n",
    "            data_element['place']['place_type'] = \"NA\"\n",
    "            data_element['place']['url'] = \"NA\"\n",
    "            data_element['place']['bounding_box'] = dict() # initialize dictionary to hold bounding box\n",
    "            data_element['place']['bounding_box']['type'] = \"Polygon\"\n",
    "            data_element['place']['bounding_box']['coordinates'] = list() # initialize coordinate list w/in bounding box\n",
    "            data_element['place']['bounding_box']['coordinates'].append([]) # append the [0] element to hold four pairs of coordinates\n",
    "            data_element['place']['bounding_box']['coordinates'][0].append([0.0, 0.0]) # append 'dummy' coordinates\n",
    "            data_element['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            data_element['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            data_element['place']['bounding_box']['coordinates'][0].append([0.0, 0.0])\n",
    "            log_array.append(data_element[\"id_str\"])\n",
    "       \n",
    "    def fix_bounding_box(self, data_element, log_array):\n",
    "        ''' Fix a few issues that are going on with bounding boxes:\n",
    "        1. Twitter Place bounding boxes only have four points. Need to close them off so they're a \n",
    "        complete polygon. Take the first coordinate of the bounding box array and repeat it at the \n",
    "        end of the bounding box array.\n",
    "        2. If the bounding box is actually a point (i.e. all of the four points are the same), then \n",
    "        \"fake out\" a bounding box by transforming into a small rectangle with a small buffer around \n",
    "        the point.  We can recognize these by looking for place.place_type == 'poi'.\n",
    "        '''\n",
    "        #print(data_element['id_str'])\n",
    "        original_bounding_box = data_element['place']['bounding_box']['coordinates'][0]\n",
    "        #print(original_bounding_box)\n",
    "        #print(data_element['place']['place_type'])\n",
    "        \n",
    "        if (data_element['place']['place_type'] == 'poi' or data_element['place']['place_type'] == 'NA'):\n",
    "            point_bounding_box = [[None for x in range(2)] for y in range(5)]\n",
    "            point_bounding_box[0][0] = original_bounding_box[0][0] - 0.0001\n",
    "            point_bounding_box[0][1] = original_bounding_box[0][1] - 0.0001\n",
    "            point_bounding_box[1][0] = original_bounding_box[1][0] - 0.0001\n",
    "            point_bounding_box[1][1] = original_bounding_box[1][1] + 0.0001\n",
    "            point_bounding_box[2][0] = original_bounding_box[2][0] + 0.0001\n",
    "            point_bounding_box[2][1] = original_bounding_box[2][1] + 0.0001\n",
    "            point_bounding_box[3][0] = original_bounding_box[3][0] + 0.0001\n",
    "            point_bounding_box[3][1] = original_bounding_box[3][1] - 0.0001\n",
    "            point_bounding_box[4][0] = original_bounding_box[0][0] - 0.0001\n",
    "            point_bounding_box[4][1] = original_bounding_box[0][1] - 0.0001\n",
    "            data_element['place']['better_bounding_box'] = dict()\n",
    "            data_element['place']['better_bounding_box']['type'] = \"Polygon\"\n",
    "            data_element['place']['better_bounding_box']['coordinates'] = list()\n",
    "            data_element['place']['better_bounding_box']['coordinates'].append([])\n",
    "            data_element['place']['better_bounding_box']['coordinates'][0] = point_bounding_box\n",
    "            #print(data_element['place']['better_bounding_box']['coordinates'])\n",
    "            log_array.append(data_element[\"id_str\"])\n",
    "        else:\n",
    "            first_coords = original_bounding_box[0]\n",
    "            #print(first_coords)\n",
    "            original_bounding_box.append(first_coords)\n",
    "            #print(original_bounding_box)\n",
    "            data_element['place']['better_bounding_box'] = dict()\n",
    "            data_element['place']['better_bounding_box']['type'] = \"Polygon\"\n",
    "            data_element['place']['better_bounding_box']['coordinates'] = list()\n",
    "            data_element['place']['better_bounding_box']['coordinates'].append([])\n",
    "            data_element['place']['better_bounding_box']['coordinates'][0] = original_bounding_box\n",
    "            #print(data_element['place']['better_bounding_box']['coordinates'])\n",
    "                  \n",
    "    def get_centroid(self, data_element):\n",
    "        bounding_box = data_element['place']['better_bounding_box']['coordinates'][0];\n",
    "        lower_left = bounding_box[0];\n",
    "        upper_right = bounding_box[2];\n",
    "        centroid_long = lower_left[0] + ((upper_right[0] - lower_left[0]) / 2);\n",
    "        centroid_lat = lower_left[1] + ((upper_right[1] - lower_left[1]) / 2);\n",
    "        data_element['place']['centroid'] = dict()\n",
    "        data_element['place']['centroid']['type'] = \"Point\"\n",
    "        data_element['place']['centroid']['coordinates'] = [centroid_long, centroid_lat]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaner = Cleaner(next_file_data, next_file_name, logs_folder)\n",
    "cleaned_data = cleaner.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145.0531355, -37.9725665]\n",
      "[-99.86000100000001, 20.6532265]\n",
      "[-56.22989, -34.819747500000005]\n"
     ]
    }
   ],
   "source": [
    "#next_file_data[122]['id_str'] # has coordinates and a place\n",
    "pprint.pprint(cleaned_data[576]['place']['centroid']['coordinates'])\n",
    "pprint.pprint(cleaned_data[4234]['place']['centroid']['coordinates'])\n",
    "pprint.pprint(cleaned_data[575]['place']['centroid']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(cleaned_data[1243])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader\n",
    "\n",
    "https://neo4j.com/developer/python/\n",
    "\n",
    "https://www.lynda.com/Neo4j-tutorials/Use-Neo4j-driver-Python/601789/659331-4.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neo4j_query_string = \"\"\"MERGE (t:Tweet {tweet_id: toInteger($tweet_id)})\n",
    "                ON CREATE SET t.text = $text,\n",
    "                    t.lang = $lang,\n",
    "                    t.timestamp_ms = toInteger($timestamp_ms),\n",
    "                    t.favorited = $favorited,\n",
    "                    t.retweeted = $retweeted,\n",
    "                    t.retweet_count = toInteger($retweet_count),\n",
    "                    t.favorite_count = toInteger($favorite_count),\n",
    "                    t.quote_count = toInteger($quote_count),\n",
    "                    t.reply_count = toInteger($reply_count),\n",
    "                    t.coordinates = point({ \n",
    "                        longitude: toFloat($tweet_coordinates_long), \n",
    "                        latitude: toFloat($tweet_coordinates_lat) \n",
    "                    })\n",
    "\n",
    "                MERGE (u:User {user_id: toInteger($user_id)})\n",
    "                SET\tu.name = $user_name,\n",
    "                    u.screen_name = $user_screen_name,\n",
    "                    u.description = $user_description,\n",
    "                    u.location = $user_location\n",
    "\n",
    "                MERGE (t)-[:TWEETED_BY]->(u)\n",
    "                MERGE (u)-[:TWEETED]->(t)\n",
    "                \n",
    "                MERGE (p:Place {place_id: toString($place_id)})\n",
    "                SET\tp.name = $place_name,\n",
    "                    p.full_name = $place_full_name,\n",
    "                    p.country = $place_country,\n",
    "                    p.country_code = $place_country_code,\n",
    "                    p.place_type = $place_type,\n",
    "                    p.bounding_box_LL = point({ \n",
    "                        longitude: toFloat($place_bounding_box_LL_long), \n",
    "                        latitude: toFloat($place_bounding_box_LL_lat) \n",
    "                    }),\n",
    "                    p.bounding_box_UR = point({ \n",
    "                        longitude: toFloat($place_bounding_box_UR_long), \n",
    "                        latitude: toFloat($place_bounding_box_UR_lat) \n",
    "                    }),\n",
    "                    p.centroid = point({ \n",
    "                        longitude: toFloat($place_centroid_long), \n",
    "                        latitude: toFloat($place_centroid_lat) \n",
    "                    })\n",
    "                \n",
    "                MERGE (t)-[:LOCATED_AT]->(p)\"\"\"\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    \n",
    "    def __init__(self, data_list, file_name, logs_path):\n",
    "        self.data_list = data_list\n",
    "        self.logs_path = logs_path\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def load_data(self):\n",
    "        driver = self.start_neo4j_session()\n",
    "        \n",
    "        begin = time.time()\n",
    "        i = 0\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            for data_element in self.data_list:\n",
    "                #print(i)\n",
    "                #print(\"Loading tweet with id: \" + data_element['id_str'] + \"; User id: \" + str(data_element['user']['id']) + \"; Place id: \" + str(data_element['place']['id']))\n",
    "                try:\n",
    "                    tx = session.begin_transaction()\n",
    "                    db_result = self.structure_data_for_load(tx, data_element)\n",
    "                    tx.commit()\n",
    "                    #db_result = session.write_transaction(self.structure_data_for_load, data_element)\n",
    "                    #print(db_result)\n",
    "                    success_count += 1\n",
    "                    #print(\"Loaded!\")\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    fail_count += 1\n",
    "                    #print(\"Couldn't load tweet with id: \" + data_element['id_str'])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            end = time.time()\n",
    "            load_time = end - begin\n",
    "\n",
    "            session.close()\n",
    "\n",
    "        self.log_load(load_time, success_count, fail_count)\n",
    "    \n",
    "    def start_neo4j_session(self):\n",
    "        # Initialize Neo4j driver\n",
    "        uri = \"bolt://localhost:7687\"\n",
    "        #user = input(\"Username: \")\n",
    "        #pwd = input(\"Password: \")\n",
    "        driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"n0sql4m3\"))\n",
    "        return(driver)\n",
    "    \n",
    "    @staticmethod\n",
    "    def structure_data_for_load(tx, data_element):\n",
    "        results = tx.run(neo4j_query_string, parameters={\n",
    "                        'tweet_id': data_element['id_str'],\n",
    "                        'text': data_element['text'],\n",
    "                        'lang': data_element['lang'],\n",
    "                        'timestamp_ms': data_element['timestamp_ms'],\n",
    "                        'favorited': data_element['favorited'],\n",
    "                        'retweeted': data_element['retweeted'],\n",
    "                        'retweet_count': data_element['retweet_count'],\n",
    "                        'favorite_count': data_element['favorite_count'],\n",
    "                        'quote_count': data_element['quote_count'],\n",
    "                        'reply_count': data_element['reply_count'],\n",
    "                        'tweet_coordinates_long': data_element['coordinates']['coordinates'][0] if data_element['coordinates'] != None else None,\n",
    "                        'tweet_coordinates_lat': data_element['coordinates']['coordinates'][1] if data_element['coordinates'] != None else None,\n",
    "                        'user_id': data_element['user']['id'],\n",
    "                        'user_name': data_element['user']['id'],\n",
    "                        'user_screen_name': data_element['user']['screen_name'],\n",
    "                        'user_description': data_element['user']['description'],\n",
    "                        'user_location': data_element['user']['location'],\n",
    "                        'place_id': data_element['place']['id'],\n",
    "                        'place_name': data_element['place']['name'],\n",
    "                        'place_full_name': data_element['place']['full_name'],\n",
    "                        'place_country': data_element['place']['country'],\n",
    "                        'place_country_code': data_element['place']['country_code'],\n",
    "                        'place_type': data_element['place']['place_type'],\n",
    "                        'place_bounding_box_LL_long': data_element['place']['better_bounding_box']['coordinates'][0][0][0],\n",
    "                        'place_bounding_box_LL_lat': data_element['place']['better_bounding_box']['coordinates'][0][0][1],\n",
    "                        'place_bounding_box_UR_long': data_element['place']['better_bounding_box']['coordinates'][0][2][0],\n",
    "                        'place_bounding_box_UR_lat': data_element['place']['better_bounding_box']['coordinates'][0][2][1],\n",
    "                        'place_centroid_long': data_element['place']['centroid']['coordinates'][0],\n",
    "                        'place_centroid_lat': data_element['place']['centroid']['coordinates'][1]\n",
    "        })\n",
    "        return(results)        \n",
    "     \n",
    "    def log_load(self, load_time, success_count, fail_count):\n",
    "        ''' If load succeeds, record the time it took to run, number of successes, and number of failures.\n",
    "        Write this info, along with file_name, as a JSON string to a log file. Then remove the name of the \n",
    "        successfully loaded file from files_to_load.txt so we don't try to re-load it on the next iteration. ''' \n",
    "        log_dict = dict()\n",
    "        log_dict['file_name'] = self.file_name\n",
    "        log_dict['load_time'] = load_time\n",
    "        log_dict['success_count'] = success_count\n",
    "        log_dict['fail_count'] = fail_count\n",
    "        loaded_files_log  = open(self.logs_path + \"/loaded_files.txt\", \"a+\") # open file in append mode\n",
    "        loaded_files_log.write(json.dumps(log_dict))\n",
    "        loaded_files_log.write(\"\\n\")\n",
    "        loaded_files_log.close()\n",
    "        \n",
    "        # https://www.reddit.com/r/learnpython/comments/3xuych/least_resource_intensive_way_to_delete_first_line/\n",
    "        files_to_load_log  = open(self.logs_path + \"/files_to_load.txt\", \"r+\") # open in read/write mode\n",
    "        files_to_load_log.readline() # read the first line and throw it out\n",
    "        remaining_files = files_to_load_log.read() # read the rest\n",
    "        files_to_load_log.seek(0) # set the cursor to the top of the file\n",
    "        files_to_load_log.write(remaining_files) # write the data back\n",
    "        files_to_load_log.truncate() # set the file size to the current size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = Loader(cleaned_data, next_file_name, logs_folder)\n",
    "loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = Extractor(data_folder, logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next file is: 500M_unicode_splitan.txt\n",
      "Reading file: /Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/500M_unicode_splitan.txt\n",
      "Read 5000 data rows.\n",
      "Next file is: 500M_unicode_splitbb.txt\n",
      "Reading file: /Users/linkalis/GIS8990_DistributedSpatialDatabases/testdatasets/data_split_5000/500M_unicode_splitbb.txt\n",
      "Read 5000 data rows.\n"
     ]
    }
   ],
   "source": [
    "while extractor.next_file_available():\n",
    "    next_file_data, next_file_name = extractor.get_next_file() # read in the next file\n",
    "    cleaner = Cleaner(next_file_data, next_file_name, logs_folder)\n",
    "    cleaned_data = cleaner.clean_data()\n",
    "    loader = Loader(cleaned_data, next_file_name, logs_folder)\n",
    "    loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-505-7cca7c542ae1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-505-7cca7c542ae1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    twurl '/1.1/geo/reverse_geocode.json?lat=2.204446&long=102.189931&granularity=country'\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# https://developer.twitter.com/en/docs/geo/places-near-location/api-reference/get-geo-reverse_geocode.html\n",
    "\n",
    "twurl '/1.1/geo/reverse_geocode.json?lat=2.204446&long=102.189931&granularity=country'\n",
    "twurl '/1.1/geo/reverse_geocode.json?lat=2.255562&long=102.250785&granularity=country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: neo4j\n",
      "Password: n0sql4m3\n",
      "<Record n=<Node id=39 labels={'Tweet'} properties={'timestamp_ms': 1514783008666, 'text': 'https://t.co/omlcNUB5kV', 'lang': 'und', 'tweet_id': 947694742837628928, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=41 labels={'Tweet'} properties={'timestamp_ms': 1514783008854, 'text': 'Happy New Years Hoe We Made It ? https://t.co/JVArFMvTBD', 'lang': 'en', 'tweet_id': 947694743626178560, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=42 labels={'Tweet'} properties={'timestamp_ms': 1514783008958, 'text': '@Co2_eSports fuck yeah lol', 'lang': 'en', 'tweet_id': 947694744062144512, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=43 labels={'Tweet'} properties={'timestamp_ms': 1514783008735, 'text': 'Feliz ano novoooo ? https://t.co/ga7QPyWjG9', 'lang': 'pt', 'tweet_id': 947694743126933505, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=45 labels={'Tweet'} properties={'timestamp_ms': 1514783009010, 'text': 'played Trees as my last/first song for the New Year two years in a row now!! I shall keep this tradition going.', 'lang': 'en', 'tweet_id': 947694744280453120, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=47 labels={'Tweet'} properties={'timestamp_ms': 1514783009050, 'text': '@sweetjonesx Bitch', 'lang': 'en', 'tweet_id': 947694744448196608, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=49 labels={'Tweet'} properties={'timestamp_ms': 1514783009064, 'text': 'FELIZ AÑO PARA TODOS??', 'lang': 'es', 'tweet_id': 947694744506916867, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=51 labels={'Tweet'} properties={'timestamp_ms': 1514783008503, 'text': '?? ??? ???? ???? ???? \\n#???_???? https://t.co/vpFNXvMgrb', 'lang': 'ar', 'tweet_id': 947694742153908224, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=52 labels={'Tweet'} properties={'tweet_id': 947694744615968768, 'coordinates': POINT(-0.8833 41.65), 'timestamp_ms': 1514783009090, 'text': 'Para mí, esto es empezar BIEN el año ?? #ÉL #siempreÉL #otroañomásatulado en Zaragoza, Spain https://t.co/RVpp71Vo1t', 'lang': 'es', 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=54 labels={'Tweet'} properties={'timestamp_ms': 1514783009100, 'text': '@Ceysin_Hoca2 6maçlar?na oynay?p kaybedelim hoca 75tlyi', 'lang': 'tr', 'tweet_id': 947694744657973248, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=63207 labels={'Tweet'} properties={'timestamp_ms': 1514782904305, 'text': 'HAPPY MEW YEAR FROM NY! @Zak_Bagans @AaronGoodwin @BillyTolley @jaywasley #GhostAdventures', 'lang': 'en', 'tweet_id': 947694305115885568, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=63208 labels={'Tweet'} properties={'timestamp_ms': 1514782904089, 'text': 'nossa senhora agpra ja sei pq meu celular ta travando https://t.co/LslEUgYfbF', 'lang': 'pt', 'tweet_id': 947694304209883136, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=63210 labels={'Tweet'} properties={'timestamp_ms': 1514782904517, 'text': 'Ayeeeeeeeee???', 'lang': 'und', 'tweet_id': 947694306005082113, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=63212 labels={'Tweet'} properties={'timestamp_ms': 1514782904661, 'text': \"It's almost 2018 and Nick just found out that Taco Bell makes tacos made with Doritos\", 'lang': 'en', 'tweet_id': 947694306608873472, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=63214 labels={'Tweet'} properties={'tweet_id': 947694306671910913, 'coordinates': POINT(-79.3971399 43.64479), 'timestamp_ms': 1514782904676, 'text': \"#happynewyear @ Fynn's of Temple Bar https://t.co/zgEDqfbiw6\", 'lang': 'en', 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=63216 labels={'Tweet'} properties={'timestamp_ms': 1514782904691, 'text': 'Happy New Year to all!!', 'lang': 'en', 'tweet_id': 947694306734862337, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=123392 labels={'Tweet'} properties={'timestamp_ms': 1514782809672, 'text': 'Happy new year! A huge shoutout to everyone in our community who made 2017 so special. We have a big plans for 2018? https://t.co/Ex41QID2Ie', 'lang': 'en', 'tweet_id': 947693908196171776, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=123393 labels={'Tweet'} properties={'timestamp_ms': 1514782809918, 'text': 'Feliz ano novo genteeeee', 'lang': 'pt', 'tweet_id': 947693909228118017, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=123394 labels={'Tweet'} properties={'timestamp_ms': 1514782809797, 'text': 'Una navidad gris, pero bueno agradezco a dios por seguir teniendo a mi lado a las personas que mas amo en este mund? https://t.co/YNbeCQXgFs', 'lang': 'es', 'tweet_id': 947693908720537600, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=123395 labels={'Tweet'} properties={'tweet_id': 947693908766605312, 'coordinates': POINT(-80.249503 26.124354), 'timestamp_ms': 1514782809808, 'text': 'Happy new year 2018 @ Broward County, Florida https://t.co/rKcYSSs1Fa', 'lang': 'en', 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=123396 labels={'Tweet'} properties={'timestamp_ms': 1514782810248, 'text': 'Até que ta engraçada', 'lang': 'pt', 'tweet_id': 947693910612172800, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=123397 labels={'Tweet'} properties={'timestamp_ms': 1514782810029, 'text': '? \\nDong Dong Dong Dong Dong Dong \\n\\n#FelizLunes', 'lang': 'en', 'tweet_id': 947693909693665280, 'favorited': False, 'retweeted': False}>>\n",
      "<Record n=<Node id=123398 labels={'Tweet'} properties={'timestamp_ms': 1514782810006, 'text': '@biaafeb Feliz 2018', 'lang': 'es', 'tweet_id': 947693909597130752, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=123399 labels={'Tweet'} properties={'timestamp_ms': 1514782809925, 'text': 'FELIZ ANO NOVO CARALHOOOOOOOOOOOO', 'lang': 'pt', 'tweet_id': 947693909257449472, 'retweeted': False, 'favorited': False}>>\n",
      "<Record n=<Node id=123400 labels={'Tweet'} properties={'timestamp_ms': 1514782808954, 'text': '@ToomeyWright Elizabeth Windsor might need to ensure that she was fully compliant with s.44.', 'lang': 'en', 'tweet_id': 947693905184669697, 'favorited': False, 'retweeted': False}>>\n"
     ]
    }
   ],
   "source": [
    "# Testing Neo4j import\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = input(\"Username: \")\n",
    "pwd = input(\"Password: \")\n",
    "driver = GraphDatabase.driver(uri, auth=(user, pwd))\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"MATCH (n:Tweet) RETURN n LIMIT 25\")\n",
    "    \n",
    "    for record in result:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
